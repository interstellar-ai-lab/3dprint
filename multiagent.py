from agents import Agent, Runner, SQLiteSession, OpenAIChatCompletionsModel, function_tool
import asyncio
from pydantic import BaseModel
from openai import AsyncOpenAI
from agents import set_tracing_disabled
import base64
from openai import OpenAI
import json
import pathlib
from PIL import Image
import io
import requests
import aiohttp
import html

import os

# API Keys for different providers
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "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")
CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY", "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")
# Add your DeepSeek and Qwen API keys here when you have them
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
QWEN_API_KEY = os.getenv("QWEN_API_KEY", "")

# OpenAI client for image generation (still needed for DALL-E)
client = OpenAI(api_key=OPENAI_API_KEY)

# Different API clients for text generation
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)

claude_client = AsyncOpenAI(
    api_key=CLAUDE_API_KEY,
    base_url="https://api.anthropic.com/v1"
)

# DeepSeek client (uncomment when you have the API key)
# deepseek_client = AsyncOpenAI(
#     api_key=DEEPSEEK_API_KEY,
#     base_url="https://api.deepseek.com/v1"
# )

# Qwen client (uncomment when you have the API key)
# qwen_client = AsyncOpenAI(
#     api_key=QWEN_API_KEY,
#     base_url="https://dashscope.aliyuncs.com/api/v1"
# )

set_tracing_disabled(disabled=True)

# Configuration for different models - easy to switch for testing
MODEL_CONFIGS = {
    "openai": {
        "client": openai_client,
        "model": "gpt-4o",
        "name": "OpenAI GPT-4o"
    },
    "claude": {
        "client": claude_client,
        "model": "claude-sonnet-4-20250514",
        "name": "Claude Sonnet 4"
    },
    # "deepseek": {
    #     "client": deepseek_client,
    #     "model": "deepseek-chat",
    #     "name": "DeepSeek Chat"
    # },
    # "qwen": {
    #     "client": qwen_client,
    #     "model": "qwen-turbo",
    #     "name": "Qwen Turbo"
    # }
}

# Current model to use for testing - change this to test different APIs
CURRENT_MODEL = "claude"  # Options: "openai", "claude", "deepseek", "qwen"

### agent 1: generation_agent
model_1 = OpenAIChatCompletionsModel(
    model="claude-sonnet-4-20250514",
    openai_client=claude_client
)
PROMPT = """
    Your task is to generate 16 views of the same object that can be used for 3D CAD reconstruction for the target object: {query}. 
    
    CRITICAL REQUIREMENTS:
    1. NO BACKGROUND: Generate images with transparent or pure white backgrounds. The object should be the only visible element.
    2. CONSISTENT SIZE: The object must appear the same size across all 16 views. Ensure the object occupies approximately the same percentage of the image frame in each view.
    3. DIVERSE ANGLES: Cover different angles and perspectives of the object for complete 3D reconstruction.
    4. CLEAN COMPOSITION: Position the object centrally in each image with adequate spacing from edges.
    
    STEP-BY-STEP PROCESS:
    1. First, create detailed prompts for 16 different views of the object
    2. For each view, call the generate_image tool with a detailed prompt
    3. Collect all the image URLs returned by the generate_image tool
    4. Create metadata describing the object and views
    5. Return everything in JSON format
    
    EXAMPLE PROMPTS FOR DIFFERENT VIEWS:
    - "A {query} viewed from the front, centered on white background, professional lighting, high detail, no shadows"
    - "A {query} viewed from the back, centered on white background, professional lighting, high detail, no shadows"
    - "A {query} viewed from the left side, centered on white background, professional lighting, high detail, no shadows"
    - "A {query} viewed from the right side, centered on white background, professional lighting, high detail, no shadows"
    - "A {query} viewed from above (top view), centered on white background, professional lighting, high detail, no shadows"
    - "A {query} viewed from below (bottom view), centered on white background, professional lighting, high detail, no shadows"
    - And 10 more views from different angles (45-degree angles, 3/4 views, etc.)
    
    IMPORTANT: You MUST call the generate_image tool 16 times to create actual images. Do not just describe the images - generate them!
    
    Return your response in this exact JSON format:
    {{
        "metadata": "Detailed description of the object and the 16 views generated",
        "image_urls": ["url1", "url2", "url3", ...], // The 16 URLs returned by generate_image tool calls
        "description": "Summary of what was generated and how it can be used for CAD reconstruction"
    }}
    """

# Custom function tool for image generation
@function_tool
def generate_image(prompt: str, quality: str = "low") -> str:
    """Generate an image using DALL-E based on the prompt"""
    try:
        img_resp = client.images.generate(
            model="dall-e-3",
            prompt=prompt,
            size="1024x1024",
            quality="hd" if quality == "high" else "standard"
        )
        # Return the image URL for now
        return img_resp.data[0].url
    except Exception as e:
        return f"Error generating image: {str(e)}"

def extract_image_urls_from_text(text: str) -> list[str]:
    """Extract image URLs from text response"""
    import re
    import json
    
    # Try to parse as JSON first
    try:
        data = json.loads(text)
        if isinstance(data, dict) and "image_urls" in data:
            return data["image_urls"]
        elif isinstance(data, list):
            # If it's a list, assume it contains URLs
            return [url for url in data if isinstance(url, str) and any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp'])]
    except (json.JSONDecodeError, TypeError):
        pass
    
    # Fallback: Look for URLs that might be image URLs using regex
    url_pattern = r'https?://[^\s<>"]+'
    urls = re.findall(url_pattern, text)
    # Filter for likely image URLs
    image_urls = [url for url in urls if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp'])]
    return image_urls

def create_image_gallery_html(image_urls: list[str], iteration: int) -> str:
    """Create HTML for displaying images in a collapsible gallery"""
    if not image_urls:
        return "<p>No images generated in this iteration.</p>"
    
    html_content = f"""
    <details style="margin: 20px 0; border: 1px solid #ddd; border-radius: 8px; padding: 10px;">
        <summary style="cursor: pointer; font-weight: bold; color: #333; padding: 10px; background-color: #f5f5f5; border-radius: 4px;">
            üì∏ Show/Hide {len(image_urls)} Generated Images (Iteration {iteration})
        </summary>
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; padding: 15px;">
    """
    
    for i, url in enumerate(image_urls):
        html_content += f"""
            <div style="border: 1px solid #eee; border-radius: 8px; padding: 10px; text-align: center;">
                <img src="{html.escape(url)}" alt="Generated Image {i+1}" style="max-width: 100%; height: auto; border-radius: 4px;">
                <p style="margin: 5px 0 0 0; font-size: 12px; color: #666;">Image {i+1}</p>
            </div>
        """
    
    html_content += """
        </div>
    </details>
    """
    return html_content

def create_3d_viewer_html(mesh_path: str, iteration: int) -> str:
    """Create HTML for 3D mesh viewer with Three.js"""
    viewer_id = f"viewer-{iteration}"
    canvas_id = f"canvas-{iteration}"
    
    html_content = f"""
    <details style="margin: 20px 0; border: 1px solid #ddd; border-radius: 8px; padding: 10px;">
        <summary style="cursor: pointer; font-weight: bold; color: #333; padding: 10px; background-color: #e8f4fd; border-radius: 4px;">
            üéÆ Show/Hide 3D Mesh Viewer (Iteration {iteration})
        </summary>
        <div class="viewer-container" id="{viewer_id}" style="position: relative; width: 100%; height: 500px; border-radius: 12px; overflow: hidden; background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); margin: 20px 0;">
            <canvas id="{canvas_id}" style="width: 100%; height: 100%; display: block;"></canvas>
            
            <div class="viewer-controls" style="position: absolute; top: 15px; right: 15px; display: flex; flex-direction: column; gap: 10px; z-index: 10;">
                <button class="viewer-control-btn" onclick="toggleRotation_{iteration}()" id="rotationBtn_{iteration}" title="Toggle Auto-Rotation" style="background: rgba(255, 255, 255, 0.9); border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; display: flex; align-items: center; justify-content: center; font-size: 18px; transition: all 0.3s ease; backdrop-filter: blur(10px);">
                    üîÑ
                </button>
                <button class="viewer-control-btn" onclick="resetCamera_{iteration}()" title="Reset Camera" style="background: rgba(255, 255, 255, 0.9); border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; display: flex; align-items: center; justify-content: center; font-size: 18px; transition: all 0.3s ease; backdrop-filter: blur(10px);">
                    üè†
                </button>
                <button class="viewer-control-btn" onclick="toggleWireframe_{iteration}()" id="wireframeBtn_{iteration}" title="Toggle Wireframe" style="background: rgba(255, 255, 255, 0.9); border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; display: flex; align-items: center; justify-content: center; font-size: 18px; transition: all 0.3s ease; backdrop-filter: blur(10px);">
                    üî≤
                </button>
                <button class="viewer-control-btn" onclick="screenshot_{iteration}()" title="Take Screenshot" style="background: rgba(255, 255, 255, 0.9); border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; display: flex; align-items: center; justify-content: center; font-size: 18px; transition: all 0.3s ease; backdrop-filter: blur(10px);">
                    üì∏
                </button>
            </div>
            
            <div class="viewer-info" id="viewerInfo_{iteration}" style="position: absolute; bottom: 15px; left: 15px; background: rgba(0, 0, 0, 0.7); color: white; padding: 10px 15px; border-radius: 8px; font-size: 14px; backdrop-filter: blur(10px);">
                Loading 3D model...
            </div>
            
            <div class="loading-overlay" id="loadingOverlay_{iteration}" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0, 0, 0, 0.8); display: flex; align-items: center; justify-content: center; color: white; font-size: 18px; z-index: 20;">
                <div class="spinner" style="width: 40px; height: 40px; border: 4px solid rgba(255, 255, 255, 0.3); border-top: 4px solid white; border-radius: 50%; animation: spin 1s linear infinite; margin-right: 15px;"></div>
                Loading 3D model...
            </div>
        </div>
        
        <div class="viewer-actions" style="display: flex; gap: 15px; margin-top: 20px; justify-content: center;">
            <button class="btn btn-secondary" onclick="downloadMesh_{iteration}()" style="padding: 10px 20px; background: #6c757d; color: white; border: none; border-radius: 8px; cursor: pointer;">
                üì• Download Mesh
            </button>
            <button class="btn" onclick="exportScreenshot_{iteration}()" style="padding: 10px 20px; background: #667eea; color: white; border: none; border-radius: 8px; cursor: pointer;">
                üì∏ Export Screenshot
            </button>
        </div>
        
        <style>
            @keyframes spin {{
                0% {{ transform: rotate(0deg); }}
                100% {{ transform: rotate(360deg); }}
            }}
            
            .viewer-control-btn:hover {{
                background: rgba(255, 255, 255, 1) !important;
                transform: scale(1.1);
            }}
            
            .viewer-control-btn.active {{
                background: #667eea !important;
                color: white !important;
            }}
        </style>
        
        <script>
            // Three.js Variables for iteration {iteration}
            let scene_{iteration}, camera_{iteration}, renderer_{iteration}, mesh_{iteration}, controls_{iteration};
            let isRotating_{iteration} = true;
            let isWireframe_{iteration} = false;
            
            // Initialize Three.js Scene for iteration {iteration}
            function initThreeJS_{iteration}() {{
                const canvas = document.getElementById('{canvas_id}');
                
                // Scene
                scene_{iteration} = new THREE.Scene();
                scene_{iteration}.background = new THREE.Color(0x1a1a2e);
                
                // Camera
                camera_{iteration} = new THREE.PerspectiveCamera(75, canvas.clientWidth / canvas.clientHeight, 0.1, 1000);
                camera_{iteration}.position.set(5, 5, 5);
                
                // Renderer
                renderer_{iteration} = new THREE.WebGLRenderer({{ canvas: canvas, antialias: true }});
                renderer_{iteration}.setSize(canvas.clientWidth, canvas.clientHeight);
                renderer_{iteration}.shadowMap.enabled = true;
                renderer_{iteration}.shadowMap.type = THREE.PCFSoftShadowMap;
                
                // Lighting
                const ambientLight = new THREE.AmbientLight(0x404040, 0.6);
                scene_{iteration}.add(ambientLight);
                
                const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
                directionalLight.position.set(10, 10, 5);
                directionalLight.castShadow = true;
                scene_{iteration}.add(directionalLight);
                
                const pointLight = new THREE.PointLight(0x667eea, 0.5, 100);
                pointLight.position.set(-10, 10, -10);
                scene_{iteration}.add(pointLight);
                
                // Grid Helper
                const gridHelper = new THREE.GridHelper(10, 10, 0x444444, 0x222222);
                scene_{iteration}.add(gridHelper);
                
                // Controls
                controls_{iteration} = new THREE.OrbitControls(camera_{iteration}, renderer_{iteration}.domElement);
                controls_{iteration}.enableDamping = true;
                controls_{iteration}.dampingFactor = 0.05;
                
                // Load the mesh
                loadOBJModel_{iteration}('{mesh_path}');
                
                // Animation Loop
                animate_{iteration}();
            }}
            
            // Animation Loop for iteration {iteration}
            function animate_{iteration}() {{
                requestAnimationFrame(animate_{iteration});
                
                if (isRotating_{iteration} && mesh_{iteration}) {{
                    mesh_{iteration}.rotation.y += 0.01;
                }}
                
                controls_{iteration}.update();
                renderer_{iteration}.render(scene_{iteration}, camera_{iteration});
            }}
            
            // Load OBJ File for iteration {iteration}
            function loadOBJModel_{iteration}(url) {{
                const loadingOverlay = document.getElementById('loadingOverlay_{iteration}');
                const viewerInfo = document.getElementById('viewerInfo_{iteration}');
                
                loadingOverlay.style.display = 'flex';
                viewerInfo.textContent = 'Loading 3D model...';
                
                // Clear existing mesh
                if (mesh_{iteration}) {{
                    scene_{iteration}.remove(mesh_{iteration});
                }}
                
                const loader = new THREE.OBJLoader();
                loader.load(
                    url,
                    function(object) {{
                        // Center and scale the model
                        const box = new THREE.Box3().setFromObject(object);
                        const center = box.getCenter(new THREE.Vector3());
                        const size = box.getSize(new THREE.Vector3());
                        const maxDim = Math.max(size.x, size.y, size.z);
                        const scale = 3 / maxDim;
                        
                        object.position.sub(center);
                        object.scale.setScalar(scale);
                        
                        // Add material
                        const material = new THREE.MeshPhongMaterial({{ 
                            color: 0x667eea,
                            shininess: 100,
                            transparent: true,
                            opacity: 0.9
                        }});
                        
                        object.traverse(function(child) {{
                            if (child.isMesh) {{
                                child.material = material;
                                child.castShadow = true;
                                child.receiveShadow = true;
                            }}
                        }});
                        
                        mesh_{iteration} = object;
                        scene_{iteration}.add(mesh_{iteration});
                        
                        // Update camera to fit model
                        const distance = maxDim * 2;
                        camera_{iteration}.position.set(distance, distance, distance);
                        controls_{iteration}.target.copy(center);
                        controls_{iteration}.update();
                        
                        loadingOverlay.style.display = 'none';
                        viewerInfo.textContent = `Model loaded: ${{mesh_{iteration}.children.length}} objects`;
                    }},
                    function(xhr) {{
                        const progress = (xhr.loaded / xhr.total * 100).toFixed(0);
                        viewerInfo.textContent = `Loading: ${{progress}}%`;
                    }},
                    function(error) {{
                        loadingOverlay.style.display = 'none';
                        viewerInfo.textContent = 'Error loading model';
                        console.error('Error loading OBJ:', error);
                    }}
                );
            }}
            
            // Control Functions for iteration {iteration}
            function toggleRotation_{iteration}() {{
                isRotating_{iteration} = !isRotating_{iteration};
                const btn = document.getElementById('rotationBtn_{iteration}');
                btn.classList.toggle('active', isRotating_{iteration});
            }}
            
            function resetCamera_{iteration}() {{
                if (mesh_{iteration}) {{
                    const box = new THREE.Box3().setFromObject(mesh_{iteration});
                    const center = box.getCenter(new THREE.Vector3());
                    const size = box.getSize(new THREE.Vector3());
                    const maxDim = Math.max(size.x, size.y, size.z);
                    const distance = maxDim * 2;
                    
                    camera_{iteration}.position.set(distance, distance, distance);
                    controls_{iteration}.target.copy(center);
                    controls_{iteration}.update();
                }}
            }}
            
            function toggleWireframe_{iteration}() {{
                isWireframe_{iteration} = !isWireframe_{iteration};
                const btn = document.getElementById('wireframeBtn_{iteration}');
                btn.classList.toggle('active', isWireframe_{iteration});
                
                if (mesh_{iteration}) {{
                    mesh_{iteration}.traverse(function(child) {{
                        if (child.isMesh) {{
                            child.material.wireframe = isWireframe_{iteration};
                        }}
                    }});
                }}
            }}
            
            function screenshot_{iteration}() {{
                renderer_{iteration}.render(scene_{iteration}, camera_{iteration});
                const canvas = renderer_{iteration}.domElement;
                const link = document.createElement('a');
                link.download = '3d-model-screenshot-{iteration}.png';
                link.href = canvas.toDataURL();
                link.click();
            }}
            
            function downloadMesh_{iteration}() {{
                window.open('{mesh_path}', '_blank');
            }}
            
            function exportScreenshot_{iteration}() {{
                screenshot_{iteration}();
            }}
            
            // Initialize Three.js when the details element is opened
            document.addEventListener('DOMContentLoaded', function() {{
                const details = document.querySelector('#{viewer_id}').closest('details');
                details.addEventListener('toggle', function() {{
                    if (this.open) {{
                        // Load Three.js libraries if not already loaded
                        if (typeof THREE === 'undefined') {{
                            loadThreeJS();
                        }} else {{
                            initThreeJS_{iteration}();
                        }}
                    }}
                }});
            }});
            
            // Load Three.js libraries
            function loadThreeJS() {{
                const scripts = [
                    'https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js',
                    'https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/OBJLoader.js',
                    'https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/controls/OrbitControls.js'
                ];
                
                let loaded = 0;
                scripts.forEach(src => {{
                    const script = document.createElement('script');
                    script.src = src;
                    script.onload = () => {{
                        loaded++;
                        if (loaded === scripts.length) {{
                            initThreeJS_{iteration}();
                        }}
                    }};
                    document.head.appendChild(script);
                }});
            }}
        </script>
    </details>
    """
    return html_content

async def download_image_to_base64(image_url: str) -> tuple[str, str]:
    """Download image from URL and convert to base64, return (base64_data, mime_type)"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(image_url) as response:
                if response.status == 200:
                    image_data = await response.read()
                    # Convert to base64
                    base64_data = base64.b64encode(image_data).decode('utf-8')
                    
                    # Detect image format from URL or content
                    mime_type = "image/jpeg"  # default
                    if image_url.lower().endswith('.png'):
                        mime_type = "image/png"
                    elif image_url.lower().endswith('.gif'):
                        mime_type = "image/gif"
                    elif image_url.lower().endswith('.webp'):
                        mime_type = "image/webp"
                    else:
                        # Try to detect from content using PIL
                        try:
                            from PIL import Image
                            import io
                            img = Image.open(io.BytesIO(image_data))
                            if img.format == 'PNG':
                                mime_type = "image/png"
                            elif img.format == 'GIF':
                                mime_type = "image/gif"
                            elif img.format == 'WEBP':
                                mime_type = "image/webp"
                        except:
                            # If detection fails, assume PNG (common for DALL-E)
                            mime_type = "image/png"
                    
                    return base64_data, mime_type
                else:
                    print(f"Failed to download image from {image_url}: {response.status}")
                    return "", ""
    except Exception as e:
        print(f"Error downloading image from {image_url}: {e}")
        return "", ""

async def download_images_locally(image_urls: list[str], session_id: str = None) -> list[str]:
    """Download images from URLs and save them locally
    
    Args:
        image_urls: List of image URLs to download
        session_id: Optional session ID for organizing files
        
    Returns:
        List of local file paths where images were saved
    """
    import pathlib
    import uuid
    import aiohttp
    
    # Create images directory
    images_dir = pathlib.Path("generated_images")
    images_dir.mkdir(exist_ok=True)
    
    # Generate session ID if not provided
    if not session_id:
        session_id = str(uuid.uuid4())[:8]
    
    session_dir = images_dir / f"session_{session_id}"
    session_dir.mkdir(exist_ok=True)
    
    print(f"üìÅ Downloading {len(image_urls)} images to: {session_dir}")
    
    local_paths = []
    
    for i, url in enumerate(image_urls):
        if not url or url.startswith("Error"):
            print(f"‚ö†Ô∏è  Skipping invalid URL {i+1}: {url}")
            local_paths.append("")
            continue
            
        try:
            print(f"‚¨áÔ∏è  Downloading image {i+1}/{len(image_urls)}...")
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        image_data = await response.read()
                        
                        # Save with descriptive filename
                        filename = f"image_{i+1:02d}.png"
                        file_path = session_dir / filename
                        
                        with open(file_path, "wb") as f:
                            f.write(image_data)
                        
                        local_paths.append(str(file_path))
                        print(f"‚úÖ Saved: {filename}")
                    else:
                        print(f"‚ùå Failed to download image {i+1}: {response.status}")
                        local_paths.append("")
                        
        except Exception as e:
            print(f"‚ùå Error downloading image {i+1}: {str(e)}")
            local_paths.append("")
    
    # Save metadata about this session
    metadata_file = session_dir / "metadata.txt"
    with open(metadata_file, "w") as f:
        f.write(f"Session ID: {session_id}\n")
        f.write(f"Downloaded: {len([p for p in local_paths if p])} images\n")
        f.write(f"Timestamp: {pathlib.Path().cwd()}\n\n")
        for i, url in enumerate(image_urls):
            f.write(f"Image {i+1}: {url}\n")
            if i < len(local_paths) and local_paths[i]:
                f.write(f"Saved as: {local_paths[i]}\n")
            f.write("\n")
    
    print(f"üìÑ Session metadata saved to: {metadata_file}")
    print(f"üéØ Downloaded {len([p for p in local_paths if p])} images successfully")
    
    return local_paths

async def generate_images_from_prompts(image_prompts: list[str]) -> tuple[list[str], list[str]]:
    """Generate images from prompts using DALL-E
    
    Returns:
        tuple: (image_urls, empty_list) - kept for compatibility
    """
    import pathlib
    import uuid
    import aiohttp
    
    image_urls = []
    
    for i, prompt in enumerate(image_prompts):
        try:
            print(f"üé® Generating image {i+1}/{len(image_prompts)}: {prompt[:50]}...")
            
            # Generate image using DALL-E
            img_resp = client.images.generate(
                model="dall-e-3",
                prompt=prompt,
                size="1024x1024",
                quality="standard"
            )
            
            image_url = img_resp.data[0].url
            image_urls.append(image_url)
            print(f"‚úÖ Generated image {i+1}: {image_url[:50]}...")
                        
        except Exception as e:
            print(f"‚ùå Error generating image {i+1}: {str(e)}")
            # Add empty string to maintain list length
            image_urls.append("")
    
    print(f"üéØ Generated {len([u for u in image_urls if u])} images successfully")
    
    return image_urls, []  # Return empty list for compatibility

def parse_evaluation_text(text: str) -> dict:
    """Parse evaluation text to extract structured data"""
    result = {
        "short_summary": "",
        "markdown_report": text,
        "suggestions_for_improvement": ""
    }
    
    # Look for "well done" in the text
    if "well done" in text.lower():
        result["suggestions_for_improvement"] = "well done"
    else:
        # Try to extract suggestions from the text
        lines = text.split('\n')
        for i, line in enumerate(lines):
            if "suggestion" in line.lower() or "improvement" in line.lower():
                suggestions = []
                for j in range(i, min(i + 5, len(lines))):
                    if lines[j].strip():
                        suggestions.append(lines[j].strip())
                result["suggestions_for_improvement"] = " ".join(suggestions)
                break
        
        if not result["suggestions_for_improvement"]:
            result["suggestions_for_improvement"] = "Continue improving the generation"
    
    # Extract short summary (first few sentences)
    sentences = text.split('.')
    if len(sentences) > 0:
        result["short_summary"] = sentences[0].strip() + "."
    
    return result

# Output schema for generation agent
class GenerationOutput(BaseModel):
    metadata: str
    """Detailed metadata about the generated views and object specifications."""
    
    image_urls: list[str]
    """List of URLs for the generated images."""
    
    description: str
    """Description of what was generated and how it can be used for CAD reconstruction."""

# Generation agent - only uses generate_image tool since mesh generation is done separately
generation_agent = Agent(
    name="Writeragent",
    instructions=PROMPT,
    model=model_1,
    tools=[generate_image]
)


### agent 2: evaluation_agent
# Agent used to synthesize a final report from the individual summaries.dog
current_config = MODEL_CONFIGS[CURRENT_MODEL]
print(f"Using {current_config['name']} for evaluation agent")

model_2 = OpenAIChatCompletionsModel(
    model=current_config["model"],
    openai_client=current_config["client"]
)
PROMPT = """
    You are an evaluation agent. 
    You need to evaluate the generated 2D images and metadata, and write a report about the evaluation.
    Answer the follow-up questions to provide hints for the next round of generation. 
    First, summarize the generated 2D images and metadata in a short 2-3 sentence summary.

    Second, you need to write a report evaluating whether these 2D images and metadata are correct/sufficient for the CAD generation task, using the following three criteria and assign a score (1-10) for each:
    1. Image Quality: Assess the visual clarity and alignment of the generated 2D images.
    2. Metadata Accuracy: Evaluate the correctness and relevance of the metadata for CAD reconstruction.
    3. Completeness: Determine if the number of views and metadata provided are sufficient for the task.  
    Include these scores in your report and provide detailed reasoning (be as specific as possible) for each score.    
     
    Third, provide suggestions for improvement. If all scores are higher than 6.5, your suggestions_for_improvement should be "well done", nothing more.
    The report should be in markdown format, and it should be detailed and comprehensive.

    """


class ReportData_2(BaseModel):
    short_summary: str
    """A short 2-3 sentence evaluation of the generated 2D images and metadata."""

    markdown_report: str
    """The final report"""

    suggestions_for_improvement: str
    """Suggestions to improve the generated 2D images and metadata."""

writer_agent = Agent(
    name="Writeragent",
    instructions=PROMPT,
    model=model_2
    # Removed output_type=ReportData_2 to avoid JSON parsing issues
)

async def generate_3d_mesh_with_llm(metadata: str, image_urls: list[str]) -> str:
    """Generate 3D mesh using GPT-4 Image model for image generation"""
    import os
    import pathlib
    import base64
    print(f"üéÆ Generating 3D mesh from {len(image_urls)} images using GPT-4 Image model...")
    
    # Validate inputs
    if not metadata or not isinstance(metadata, str):
        print("‚ùå ERROR: Invalid metadata parameter")
        return "Error: Invalid metadata"
    
    if not image_urls or not isinstance(image_urls, list):
        print("‚ùå ERROR: Invalid image_urls parameter")
        return "Error: Invalid image_urls"
    
    if len(image_urls) == 0:
        print("‚ùå ERROR: No image URLs provided for mesh generation")
        return "Error: No images available for mesh generation"
    
    try:
        # Create output directory
        out_dir = pathlib.Path("mesh_outputs")
        out_dir.mkdir(exist_ok=True)
        
        # Generate a unique filename
        import uuid
        mesh_id = str(uuid.uuid4())[:8]
        mesh_filename = f"mesh_llm_{mesh_id}.obj"
        mesh_path = out_dir / mesh_filename
        
        # Create the prompt using the image URLs instead of embedding base64 data
        prompt = f"""
        Give me the 3D mesh for the object in the images. 
        The images include multi-views of the object, and the corresponding metadata can be found in: {metadata}. 
        Generate a detailed 3D mesh visualization based on the object shown in the images.
        """
        
        # Use GPT-4 Image model to generate a mesh visualization
        img_resp = client.images.generate(
            model="gpt-image-1",
            prompt=prompt,
            size="1024x1024",
        )
        
        # Get the generated image data
        mesh_data = img_resp.data[0].b64_json
        
        # Save the mesh visualization image
        png_path = out_dir / f"mesh_llm_{mesh_id}.png"
        with open(png_path, "wb") as f:
            f.write(base64.b64decode(mesh_data))
        
        print(f"‚úÖ Generated mesh visualization: {png_path}")
        
        # Create OBJ file with reference to the generated visualization
        obj_content = f"""# 3D Mesh Visualization Generated by GPT-4 Image Model
# Generated from {len(image_urls)} images
# Metadata: {metadata}
# Visualization saved as: {png_path}

# This is a reference to the generated mesh visualization
# The actual 3D mesh is represented as an image: {png_path}

# Simple cube mesh as placeholder (actual mesh is in the generated image)
v -1.0 -1.0 -1.0
v 1.0 -1.0 -1.0
v 1.0 1.0 -1.0
v -1.0 1.0 -1.0
v -1.0 -1.0 1.0
v 1.0 -1.0 1.0
v 1.0 1.0 1.0
v -1.0 1.0 1.0
vn 0.0 0.0 -1.0
vn 0.0 0.0 1.0
vn 0.0 -1.0 0.0
vn 0.0 1.0 0.0
vn -1.0 0.0 0.0
vn 1.0 0.0 0.0
f 1//1 2//1 3//1 4//1
f 5//2 8//2 7//2 6//2
f 1//3 5//3 6//3 2//3
f 2//4 6//4 7//4 3//4
f 3//5 7//5 8//5 4//5
f 5//6 1//6 4//6 8//6"""
        
        # Save the OBJ file
        with open(mesh_path, "w") as f:
            f.write(obj_content)
        
        print(f"‚úÖ Successfully generated mesh with GPT-4 Image model: {mesh_path}")
        return str(mesh_path)
        
    except Exception as e:
        print(f"‚ùå Error generating mesh with GPT-4 Image model: {str(e)}")
        return f"Error generating mesh: {str(e)}"

# Create a session instance
session = SQLiteSession("conversation_123")

# Function to switch models for testing
def switch_model(model_name):
    """Switch to a different model for testing"""
    global CURRENT_MODEL, model_2, writer_agent
    if model_name in MODEL_CONFIGS:
        CURRENT_MODEL = model_name
        current_config = MODEL_CONFIGS[model_name]
        print(f"Switching to {current_config['name']}")
        
        model_2 = OpenAIChatCompletionsModel(
            model=current_config["model"],
            openai_client=current_config["client"]
        )
        
        writer_agent = Agent(
            name="Writeragent",
            instructions=PROMPT,
            model=model_2
            # Removed output_type=ReportData_2 to avoid JSON parsing issues
        )
        return True
    else:
        print(f"Model {model_name} not found. Available models: {list(MODEL_CONFIGS.keys())}")
        return False

async def main():
    query = input("What would you like to generate? ")
    print(query)
    
    # Ask which model to use for testing
    print(f"\nAvailable models for testing:")
    for key, config in MODEL_CONFIGS.items():
        print(f"  {key}: {config['name']}")
    
    model_choice = input(f"\nWhich model to use? (default: {CURRENT_MODEL}): ").strip().lower()
    if model_choice and model_choice in MODEL_CONFIGS:
        switch_model(model_choice)
    
    suggestions = ""
    iteration = 0
    
    # Step 1: Generate metadata and image prompts
    result_1 = await Runner.run(generation_agent, f"Please generate the materials needed for 3D CAD generation for: {query}", session=session)
    
    # Parse the structured output
    try:
        # Try to parse as JSON first
        import json
        parsed_output = json.loads(result_1.final_output)
        metadata = parsed_output.get("metadata", "")
        image_prompts = parsed_output.get("image_prompts", [])
    except (json.JSONDecodeError, KeyError):
        # Fallback to extracting from text
        metadata = result_1.final_output
        image_prompts = []
    
    # Step 2: Generate images from prompts
    image_urls = []
    if image_prompts:
        print(f"\nGenerating {len(image_prompts)} images from prompts...")
        image_urls, _ = await generate_images_from_prompts(image_prompts)
    else:
        # Fallback: extract URLs from text if prompts weren't found
        image_urls = extract_image_urls_from_text(metadata)
    
    # Step 3: Download images locally
    local_image_paths = []
    if image_urls:
        print(f"\nDownloading {len(image_urls)} images locally...")
        local_image_paths = await download_images_locally(image_urls, "main_session")
    # Download and convert first image to base64 if available
    b64_image = ""
    b64_mime_type = "image/jpeg"
    if image_urls:
        b64_image, b64_mime_type = await download_image_to_base64(image_urls[0])
    
    while suggestions != "well done":
        iteration += 1
        # First turn
        contents = [
            {
                "role": "user",
                "content": f"The generated metadata is: {metadata}.",
            },
            {
                "role": "user",
                "content": "Please evaluate the generated 2D images and metadata, and think about the follow-up questions according to the guidelines.",
            }            
        ]
        
        # Only add image content if we have a valid base64 image
        if b64_image and b64_image.strip():
            contents.insert(0, {
                "role": "user",
                "content": [
                    {
                        "type": "input_image",
                        "detail": "auto",
                        "image_url": f"data:{b64_mime_type};base64,{b64_image}",
                    }
                ],
            })
        result_2 = await Runner.run(writer_agent, contents)
        evaluation_text = result_2.final_output
        
        # Parse the evaluation text manually
        parsed_evaluation = parse_evaluation_text(evaluation_text)
        suggestions = parsed_evaluation["suggestions_for_improvement"]
        
        new_prompt = (
            f"The target for your generation is: {query}. Detailed task is introduced by the instructions from the system above.\n\n"
            f"Below are the metadata from your previous generation attempt:\n\n{metadata}\n\n"
            f"Please refine the generation results based on the system instructions. Pay special attention to the following suggestions for improvement: {suggestions}.\n\n"
            f"Additionally, consider the scores and reasoning provided in the evaluation report for the previous generation attempt:\n\n{parsed_evaluation['markdown_report']}."
        )
        
        # Generate improved version with new prompts and images
        result_1 = await Runner.run(generation_agent, new_prompt, session=session)
        
        # Parse the structured output again
        try:
            parsed_output = json.loads(result_1.final_output)
            metadata = parsed_output.get("metadata", "")
            image_prompts = parsed_output.get("image_prompts", [])
            description = parsed_output.get("description", "")
        except (json.JSONDecodeError, KeyError):
            metadata = result_1.final_output
            image_prompts = []
            description = ""
        
        # Generate new images from prompts
        if image_prompts:
            print(f"\nGenerating {len(image_prompts)} new images from prompts...")
            image_urls, _ = await generate_images_from_prompts(image_prompts)
        else:
            image_urls = extract_image_urls_from_text(metadata)
        
        # Download new images locally
        local_image_paths = []
        if image_urls:
            print(f"\nDownloading {len(image_urls)} new images locally...")
            local_image_paths = await download_images_locally(image_urls, f"main_session_iter_{iteration}")
        # Download and convert first image to base64 if available
        b64_image = ""
        b64_mime_type = "image/jpeg"
        if image_urls:
            b64_image, b64_mime_type = await download_image_to_base64(image_urls[0])
        # Second turn - agent automatically remembers previous context
        # Save the evaluation results separately for each iteration
        out_dir = pathlib.Path(f"evaluation_reports_{iteration}"); out_dir.mkdir(exist_ok=True)
        
        # Generate mesh for this iteration using LLM
        mesh_path = await generate_3d_mesh_with_llm(metadata, image_urls)
        
        # Create HTML report with images and 3D viewer
        html_report = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Iteration {iteration} Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f0f0; padding: 15px; border-radius: 8px; margin-bottom: 20px; }}
                .content {{ margin: 20px 0; }}
                .evaluation {{ background-color: #f9f9f9; padding: 15px; border-radius: 8px; margin: 20px 0; }}
                .section {{ margin: 30px 0; }}
                .section h2 {{ color: #2c5aa0; border-bottom: 2px solid #2c5aa0; padding-bottom: 10px; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>üéØ Iteration {iteration} Report</h1>
                <p><strong>Query:</strong> {query}</p>
                <p><strong>Generated {len(image_urls)} images</strong></p>
                <p><strong>3D Mesh:</strong> {pathlib.Path(mesh_path).name}</p>
            </div>
            
            <div class="section">
                <h2>üì∏ Generated Images</h2>
                {create_image_gallery_html(image_urls, iteration)}
            </div>
            
            <div class="section">
                <h2>üéÆ 3D Mesh Viewer</h2>
                {create_3d_viewer_html(mesh_path, iteration)}
            </div>
            
            <div class="evaluation">
                <h2>üìä Evaluation Report</h2>
                <h3>Short Summary</h3>
                <p>{parsed_evaluation["short_summary"]}</p>
                
                <h3>Detailed Report</h3>
                <div style="white-space: pre-wrap;">{parsed_evaluation["markdown_report"]}</div>
                
                <h3>Suggestions for Improvement</h3>
                <p><strong>{parsed_evaluation["suggestions_for_improvement"]}</strong></p>
            </div>
        </body>
        </html>
        """
        
        # Save HTML report
        html_path = out_dir / f"iteration_{iteration}_report.html"
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(html_report)
        
        # Save markdown report
        markdown_path = out_dir / f"iteration_{iteration}_report.md"
        with open(markdown_path, "w") as f:
            f.write(parsed_evaluation["markdown_report"])
        
        # Save suggestions for improvement
        suggestions_path = out_dir / f"iteration_{iteration}_suggestions.txt"
        with open(suggestions_path, "w") as f:
            f.write(parsed_evaluation["suggestions_for_improvement"])
        
        # Save short summary
        summary_path = out_dir / f"iteration_{iteration}_summary.txt"
        with open(summary_path, "w") as f:
            f.write(parsed_evaluation["short_summary"])
        
        # Print the markdown report to the console
        print(f"\n{'='*60}")
        print(f"ITERATION {iteration} COMPLETED")
        print(f"{'='*60}")
        print(f"Generated {len(image_urls)} images")
        print(f"Created 3D mesh: {pathlib.Path(mesh_path).name}")
        print(f"HTML report with 3D viewer saved to: {html_path}")
        print(f"Markdown report saved to: {markdown_path}")
        print(f"\nEvaluation Summary: {parsed_evaluation['short_summary']}")
        print(f"Suggestions: {parsed_evaluation['suggestions_for_improvement']}")
        print(f"\n{parsed_evaluation['markdown_report']}")
        print(f"{'='*60}\n")
    print("All iterations completed. Final report saved in", out_dir)
    
    # Create a final summary HTML file
    final_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Final Report - {query}</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            .header {{ background-color: #e8f5e8; padding: 20px; border-radius: 8px; margin-bottom: 20px; }}
            .iteration {{ margin: 20px 0; border: 1px solid #ddd; border-radius: 8px; padding: 15px; }}
            .iteration h3 {{ color: #2c5aa0; }}
            .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }}
            .stat-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; border-left: 4px solid #667eea; }}
            .stat-number {{ font-size: 2em; font-weight: bold; color: #667eea; }}
            .stat-label {{ color: #666; margin-top: 5px; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>üéâ Final Report - {query}</h1>
            <p><strong>Total Iterations:</strong> {iteration}</p>
            <p><strong>Status:</strong> ‚úÖ Task completed successfully!</p>
        </div>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number">{iteration}</div>
                <div class="stat-label">Total Iterations</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{iteration * 16}</div>
                <div class="stat-label">Total Images Generated</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{iteration}</div>
                <div class="stat-label">3D Meshes Created</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">üéÆ</div>
                <div class="stat-label">Interactive 3D Viewers</div>
            </div>
        </div>
        
        <h2>üìÅ Generated Reports</h2>
        <p>Each iteration has its own folder with detailed reports, images, and interactive 3D mesh viewers:</p>
        <ul>
    """
    
    for i in range(1, iteration + 1):
        final_html += f"""
            <li><strong>Iteration {i}:</strong> 
                <a href="evaluation_reports_{i}/iteration_{i}_report.html" target="_blank">üéÆ View Interactive Report</a> | 
                <a href="evaluation_reports_{i}/iteration_{i}_report.md" target="_blank">üìÑ View Markdown Report</a> |
                <a href="mesh_outputs/mesh_{i}.obj" target="_blank">üì¶ Download 3D Mesh</a>
            </li>
        """
    
    final_html += """
        </ul>
        
        <div class="iteration">
            <h3>üìã Final Summary</h3>
            <p>The multi-agent system has successfully completed the 3D CAD generation task through iterative improvement.</p>
            <p>Each iteration generated 16 images and was evaluated for quality, accuracy, and completeness.</p>
            <p>üéÆ <strong>Interactive 3D Viewers:</strong> Each iteration includes a fully interactive 3D mesh viewer with:</p>
            <ul>
                <li>üîÑ Auto-rotation and manual camera controls</li>
                <li>üî≤ Wireframe toggle for detailed inspection</li>
                <li>üì∏ Screenshot capture functionality</li>
                <li>üì• Direct mesh download capability</li>
                <li>üé® Professional lighting and materials</li>
            </ul>
            <p>Check the individual iteration reports above to see all generated images, 3D meshes, and detailed evaluations.</p>
        </div>
    </body>
    </html>
    """
    
    # Save final summary
    final_path = pathlib.Path("final_report.html")
    with open(final_path, "w", encoding="utf-8") as f:
        f.write(final_html)
    
    print(f"\nüéâ FINAL SUMMARY")
    print(f"{'='*60}")
    print(f"‚úÖ Task completed in {iteration} iterations")
    print(f"üìÅ Final report: {final_path}")
    print(f"üì∏ Each iteration generated and displayed 16 images")
    print(f"üéÆ Each iteration includes interactive 3D mesh viewer")
    print(f"üì¶ 3D mesh files saved in mesh_outputs/ folder")
    print(f"üìä Detailed reports saved in evaluation_reports_* folders")
    print(f"{'='*60}")
    
    print("‚úÖ All mesh data generated and saved successfully.")

if __name__ == "__main__":
    asyncio.run(main())